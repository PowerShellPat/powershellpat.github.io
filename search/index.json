[{"content":"Introduction AWS networking is fairly straightforward in a small setup. As a singular environment, you will generally have a single VPC that consists of:\nSubnets Route tables Network ACLs (NACLs) Security groups An Internet Gateway (IGW) NAT Gateway(s) There are other components, and even some of the ones mentioned here may not be required. You might have a single subnet for all of your resources. Likewise, a NAT Gateway may not even be necessary (this is rare — and anyone exposing private workloads to the internet unnecessarily is not following best practice).\nBut what about in more sophisticated setups, where you have multiple isolated networks in the form of several VPCs across different AWS accounts? You may need to introduce a shared services layer — an isolated VPC (or multiple, depending on your use case) that houses applications used to support your environment. Rather than duplicating these components inside each VPC, we share them (hence the name). To share these components, the VPCs need to be interconnected. AWS provides a service called Transit Gateway (TGW) to achieve exactly that, at scale.\nBuilding on the shared services concept, NAT Gateways provide a solution that allows private subnets to connect to the internet for outbound (egress) traffic. They are deployed into public subnets (with a default route to an IGW) so that resources can route through the NAT Gateway to access the internet. NAT Gateways aren’t strictly bound to the VPC they are deployed in — you can use them across different VPCs if you choose, by leveraging a TGW.\nWith all of this context in mind, at what point does it make sense to use a NAT Gateway in each VPC (decentralised egress) versus using a TGW to route all internet traffic through a single VPC and set of NAT Gateways (centralised egress)? Which option is more cost-effective? Let’s find out.\nArchitecture: Centralised vs decentralised Let’s begin by outlining each of the solutions and what they look like architecturally.\nCentralised VPC count: 4 Workload VPCs: 3 Egress VPC: 1 Transit Gateway count: 1 Transit Gateway attachments: 4 Transit Gateway route tables: 1 NAT Gateway count: 2 (both deployed into the Egress VPC) Each VPC will be associated with a single Transit Gateway (TGW) route table and propagate its routes accordingly. A default route will be configured to point to the egress VPC, enabling a centralised egress pattern.\nEach VPC will be created in a separate AWS account, except for the egress VPC, which (along with the TGW) will reside in a dedicated networking account. The TGW will be shared via AWS Resource Access Manager (RAM) to each respective AWS account, allowing TGW attachments to be created for each VPC.\nThis is what the architecture looks like: Decentralised VPC count: 3 (Workload VPCs only) Transit Gateway count: 1 Transit Gateway attachments: 3 Transit Gateway route tables: 1 NAT Gateway count: 6 (2 in each workload VPC) Each VPC will be associated with a single TGW route table and will propagate its routes accordingly. This route table will not contain a default route, as traffic will exit locally via each VPCs NAT Gateways before reaching the TGW.\nEach VPC will be created in separate AWS accounts. The TGW will again be created in the networking account and RAM shared to each AWS account to allow for TGW attachments per VPC.\nThis is what the architecture looks like: Now that the architectures are defined, let’s dig deeper into the pricing of each component.\nPricing models NAT Gateways NAT Gateways have a simple pricing model. You pay for two main things:\nHourly cost per NAT Gateway Data processed (in GB) by the NAT Gateway The source or destination of the traffic is irrelevant — if the data passes through the NAT Gateway, you\u0026rsquo;re paying for it.\nIn addition to NAT Gateway costs, there are data transfer fees for services like EC2. This includes:\nTraffic going out to the internet Traffic sent to other AWS regions Traffic sent between Availability Zones (AZs) within the same region It’s also worth mentioning that VPCs themselves are free. While components inside a VPC (like NAT Gateways) may incur costs, the VPC construct has no cost on its own. So for this comparison, we’ll only factor in the pricing for NAT Gateways.\nTransit Gateways Transit Gateways follow a similar pricing model to NAT Gateways. You pay for:\nHourly cost per Transit Gateway attachment Data processed (in GB) by the Transit Gateway As with NAT Gateways, data transfer fees for services like EC2 still apply and should be considered — especially for inter-AZ or inter-region traffic.\nCost breakdown To calculate actual costs, we need to use the pricing from an AWS region. For this exercise, we’ll use the AWS Sydney region (ap-southeast-2). Based on this region, the pricing is as follows:\nNAT Gateway Hourly fee: $0.059 Per GB of data processed: $0.059 Transit Gateway: Hourly fee (per attachment): $0.07 Per GB of data processed: $0.02 For both the centralised and decentralised solutions already outlined, we’ll assume 100 GB of data processed in total. We’ll also assume a billing duration of 30 days (720 hours) for the hourly costs.\nAlthough both solutions will incur data transfer fees (e.g. to the internet, to other AWS regions, or between AZs within a region), these are identical in both cases. As such, we’ve excluded them from this comparison to focus only on the NAT and Transit Gateway pricing.\nCentralised cost breakdown Component Cost NAT Gateway 1 (hourly fee) $42.48 NAT Gateway 2 (hourly fee) $42.48 NAT Gateway (data processing) $5.90 Transit Gateway (Egress VPC attachment) $50.40 Transit Gateway (Workload A VPC attachment) $50.40 Transit Gateway (Workload B VPC attachment) $50.40 Transit Gateway (Workload C VPC attachment) $50.40 Transit Gateway (data processing) $2 Total $294.46 Decentralised cost breakdown Component Cost NAT Gateway 1 (hourly fee) $42.48 NAT Gateway 2 (hourly fee) $42.48 NAT Gateway 3 (hourly fee) $42.48 NAT Gateway 4 (hourly fee) $42.48 NAT Gateway 5 (hourly fee) $42.48 NAT Gateway 6 (hourly fee) $42.48 NAT Gateway (data processing) $5.90 Transit Gateway (Workload A VPC attachment) $50.40 Transit Gateway (Workload B VPC attachment) $50.40 Transit Gateway (Workload C VPC attachment) $50.40 Transit Gateway (data processing) $2 Total $413.98 Findings With the findings in tow, it\u0026rsquo;s clear which solution is more cost-effective based on the defined setup. The centralised egress solution comes out ahead in terms of cost when compared to the decentralised one — as long as the assumptions hold.\nHowever, there are edge cases worth exploring. Also, note that we used a \u0026ldquo;magic number\u0026rdquo; of 6 NAT Gateways in the decentralised setup — something we’ll expand on shortly.\nWhat if fewer VPCs needed egress? Suppose only one of the three workload VPCs in the decentralised solution actually required egress access (and therefore NAT Gateways). If you removed 4 of the 6 NAT Gateways, your total would drop to $244.06 — cheaper than the centralised solution.\nWhat If You Removed the Transit Gateway? Now imagine removing the Transit Gateway from the decentralised solution entirely, and each of the three VPCs had their own local NAT Gateways, but no interconnection. In that case, the total cost would come to $260.78 — not as low as the previous example, but still cheaper than the centralised solution (at $294.46).\nWhy We Used 3 VPCs as a Baseline The reason we compared three workload VPCs and 6 NAT Gateways is that this configuration represents a cost-efficiency threshold. Beyond this point, the decentralised approach starts to suffer from diminishing returns.\nWhile it may seem obvious to adopt a centralised solution, there\u0026rsquo;s a case to be made for starting with decentralised egress — especially in early-stage AWS environments that aren’t yet interconnected. If the VPCs remain isolated, decentralised egress might be more cost-effective. But as soon as inter-VPC connectivity is required, the decentralised model becomes significantly less economical.\nAdditional cost scenarios Here are some expanded scenarios to help illustrate the cost impact at scale:\nScenario Total Cost Decentralised: 4 VPCs, 8 NAT Gateways, 0 TGW $345.74 Centralised: 4 VPCs + 1 egress VPC, 2 NAT Gateways, 5 TGW attachments $344.86 Decentralised: 5 VPCs, 10 NAT Gateways, 0 TGW $430.70 Centralised: 5 VPCs + 1 egress VPC, 2 NAT Gateways, 6 TGW attachments $395.26 As shown above, once you pass three VPCs, the centralised model quickly becomes more attractive from a cost perspective.\nCentralised solution benefits While the primary focus of this blog is cost, not everything comes down to price. Ultimately, the real question is: what are you willing to pay for best practice and a secure, scalable setup?\nAWS provides several services that enhance security at the networking layer. Two key examples include:\nAWS Network Firewall – a fully managed stateful firewall service Gateway Load Balancer (GWLB) – supports third-party firewalls in a “bump-in-the-wire” architecture Both of these services are designed to operate in a centralised model, enabling consistent inspection of traffic — whether it\u0026rsquo;s north/south (in and out of the AWS network) or east/west (between VPCs/subnets). Any traffic that bypasses these centralised inspection points risks being unauthorised or even malicious. Without a centralised architecture, enforcing an inspection solution across multiple VPCs becomes near impossible.\nIf you needed any more reasons to lean toward a centralised model — regardless of cost — the ability to integrate services like AWS Network Firewall and Gateway Load Balancer should be near the top of the list.\nConclusion Egress traffic in AWS must traverse some kind of gateway to reach the internet. For private subnets, this means using a NAT Gateway. As we’ve explored, NAT Gateways can either be local to each VPC or located in a centralised egress VPC, connected via a Transit Gateway.\nCentralised solutions work best at scale and offer stronger alignment with AWS’s security services. Decentralised solutions, on the other hand, are often more suitable for smaller environments — either early in their cloud journey or intentionally minimal in scope.\nThe next time you\u0026rsquo;re designing how traffic should leave your AWS environment, consider the long-term direction, cost implications, security posture, and — most importantly — what the environment needs to function effectively.\nCover image generated by DeepAI\n","date":"2025-07-08T22:00:32+10:00","image":"https://powershellpat.github.io/p/designing-egress-in-aws/cover_hu_c9a9f3a0b72e2f4.jpeg","permalink":"https://powershellpat.github.io/p/designing-egress-in-aws/","title":"Designing Egress in AWS"},{"content":"Introduction Designing IT solutions is a challenging task. Not only do you need to take several factors into consideration, but you also need to understand the lifecycle of what you’re designing. Questions will inevitably pop into your head, like:\nHow long does this solution need to last? Is it going to be perpetual? How do we handle high availability and disaster recovery? When will I know it\u0026rsquo;s complete? That last point—when will I know it\u0026rsquo;s complete?—can be ambiguous. Does it mean the solution is fully fleshed out and working at the desired end state? Or does it mean it\u0026rsquo;s ready once a Minimum Viable Product (MVP) has been constructed?\nIf the solution is too simple, it may lack features or fail to scale. But if it’s too complex—over-engineered—it might introduce unexpected bugs and hurt not just our credibility, but also the brand we represent, especially in a corporate context. And beyond the bugs, what about the cost of building that solution? Our time? Money? Resources? All potentially wasted on something that doesn’t deliver a meaningful return on investment.\nIt’s a conundrum, no doubt. But there are frameworks and considerations that help you draw the line and confidently mark a solution as done based on criteria.\nUnderstanding a solution One of the first fundamental skills that you need to understand as a Consultant or anyone in a role that involves designing or architecting solutions—is the ability to understand the what and the why:\nWhat are we doing? What are we building? What is it? What will it be? Once we understand that what, we move on to the why:\nWhy are we doing this? Why are we building this? Why do we need to include x, y and z? Why is this or that a good idea? All of those what and why questions can be summarised in a single word: requirements. Requirements are the driving force behind the rationale. They help shape what the solution will look like and give context to the decisions made along the way. They set the direction—establishing goals, coordinating efforts across teams, and managing expectations for stakeholders. From a commercial perspective, requirements are assessed and translated into elements such as definition of done and/or acceptance criteria. These elements are then used in commercial agreements like a Statement of Work (SoW) to provide clear and concise deliverables for all parties involved.\nWhen we understand the requirements of what we’re setting out to do, we have structure. That structure becomes the foundation for a plan—where ideas take shape and strategy begins to form. As you become more proficient in your craft—whether it’s software, infrastructure, or something else—you’ll begin to recognise patterns. You’ll recall what worked well and what didn’t in previous experiences. Armed with that insight, you’ll feel more confident when similar situations arise, and better equipped to act early—before issues have a chance to fester.\nThe definition of complete Now comes the tricky part of this process: What defines a solution as complete?\nIs it something that we can measure? Does it relate to its run cost? Can it handle the load we need it to? Will it accumulate technical debt? We do have frameworks and tools to help answer these questions. In the previous section on understanding a solution, we mentioned the definition of done and acceptance criteria. Depending on how mature the environment and organisation are, these could be loosely defined or meticulously detailed. By using these elements, we can clearly describe what a “complete” solution looks like—aligned with what we’re trying to achieve now.\nEven though we use the term “complete”, it doesn’t always mean the end state. More often, it marks a meaningful checkpoint. The future state will continue to evolve—just as the broader IT landscape constantly shifts.\nA definition of done may reflect a simple, even traditional, solution. Solutions don’t need to be extravagant if they fulfill the basic requirements. There’s nothing wrong with doing things the old-fashioned way. Take mainframes, for example. You might think of them as legacy tech from the 1960s—but would it surprise you to know that many major corporations across sectors like finance, government, and healthcare still rely on mainframes today to process transactions? Mainframes continue to hold relevance because of their unique capabilities and the critical roles they play in these industries. Could they be replaced by modern stacks like serverless architectures? Probably. Is it a simple task? Absolutely not.\nThe mainframe example highlights two key principles:\nIf it isn’t broken, don’t fix it — or in other words, if it works, don’t change it. Old technology doesn’t automatically mean obsolete — in fact, as older generations retire, we might even see a kind of renaissance for legacy technologies like mainframes. At one point in history, mainframes were considered the north star of computing. Fast forward to today, and our modern-day north stars might be serverless platforms that run only your code—or perhaps AI-powered observability systems that proactively detect anomalies.\nTechnology doesn’t stand still. It evolves rapidly. Today, we’re seeing explosive growth in AI, particularly with agentic models that leverage Model Context Protocol (MCP) servers.\nSo when it comes to defining what makes a solution \u0026ldquo;complete,\u0026rdquo; remember these quick tidbits:\nIt’s fit for purpose. It doesn’t need to last forever—just for today, tomorrow, and the near future. Solutions will always evolve. It will scale within reason. If you’re expecting millions of requests per second, architect accordingly. But if you\u0026rsquo;re only handling a few per second, you don’t need a massive data platform with all the bells and whistles that might cost thousands per month to run. It will work. This is the core of it all. “Working” might mean different things to different people, but ultimately, the solution behaves as intended and delivers on its purpose. Over-engineering and its ramifications So what happens when we go to the other extreme? Rather than stopping at our definition of done, we push way past it and engineer the hell out of a solution. It’s amazing! It’s something to behold!\n…Or is it?\nHow far down the rabbit hole did you have to go to get there? And ultimately—what was the cost?\nOver-engineering is contextual. When it does happen, it can manifest in different forms depending on the solution. One of the worst forms is trying to account for every possible future use case—a “just in case” mindset. Building a solution to handle everything that might happen in the future can stifle progress. It can halt or delay the project until there is mutual resolution amongst stakeholders, all while delaying progress to the end goal. In situations like this, I often use the saying: “the juice isn’t worth the squeeze.” In other words, to achieve X, we need to do Y—and Y just isn’t worth the investment of our time, resources, or mental energy.\nAnother form of over-engineering is adding unnecessary complexity. The maturity of a solution must be factored in when architecting. Introducing advanced complexity to a system that’s not ready for it can be wasteful, or even damaging. Take a startup, for example. Let’s say their core revenue-driving product is a piece of software. Right now, that software runs on a single PC under someone’s desk. Is it elegant? No. Does it work for now? Actually, yes—in a very non-ideal way.\nIf that same startup tries to launch into a cutting-edge, serverless cloud architecture with auto-scaling, CI/CD pipelines, and bleeding-edge automation—but has no current need for it—that effort would be a catastrophe. The resources required to build and support such a setup would likely never see a return, especially if the startup is still finding product-market fit or struggling with cash flow. In fact, they might run out of steam entirely and fold before realising their goals.\nA solution shouldn’t be too top-heavy at the start. By that, I mean: don’t add features or components that don’t bring actual value in the near term. Later—once things stabilize and there’s a clear path forward—it might make sense to revisit and layer in those “nice to have” features. But early on, they often just weigh you down.\nReturn on Investment (ROI) and building momentum After we get through the rigorous process of vetting the solution using the mechanisms already mentioned, we need to shift into a mindset that focuses on vision—the long-term outlook for the solution. As part of this vision, we must consider how we extract value over time and achieve a solid return on investment (ROI). But ROI isn’t always about money. There are other forms of currency we “spend” to make a solution functional and successful. These include:\nTime Effort On-going support Business integration Let’s start with time. It’s one of our most precious resources—yet most of us (myself included) never seem to have enough of it. In the digital age, it’s easy to jump from watching Judge Judy on YouTube to checking sports results, reading the latest tariff news, playing a video game, or falling into a social media rabbit hole. There’s always something waiting to eat into our time.\nSo, when we invest time into a solution, it should give us time back. The return should come in the form of efficiency or automation. This is especially true when adopting DevOps methodologies, where the whole premise is to automate as much of your role as possible—freeing you up for higher-value tasks.\nAnother often-overlooked factor is the longevity of the solution and what ongoing support and maintenance will look like. Simple systems might be seen as ancient by today’s standards—but they can also be the easiest to maintain, debug, and enhance. A “good enough” solution today often beats a “perfect” one six months from now.\nA solution that is well-vetted, tested, iterated upon, intuitive, and clearly defined delivers strong ROI—not just for development teams, but also for project managers, business stakeholders, and end users. When more people begin interacting with the solution, momentum builds. Feedback loops form, giving valuable insight into what to improve or prioritise next.\nThis is how momentum turns into progress. This is how “good enough” evolves into great.\nConclusion The next time you\u0026rsquo;re designing a solution—or playing a role in shaping one—consider what’s practical to get it working now, versus what’s “perfect” but loaded with nice-to-have features that may not add much value.\nDon’t get stuck in the weeds and chase those rabbits too deep into those holes! While it can be fun to experiment and theory-craft, a good engineer or architect knows when they\u0026rsquo;ve gone deep enough to achieve a solid result for a reasonable amount of effort. In the end, completing a solution up until a point isn’t settling. It\u0026rsquo;s being strategic.\nCover image by dix sept on Unsplash\n","date":"2025-06-12T22:20:32+10:00","image":"https://images.unsplash.com/photo-1624961151169-b3df5c0f06ab?q=80\u0026w=882\u0026auto=format\u0026fit=crop\u0026ixlib=rb-4.1.0\u0026ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D","permalink":"https://powershellpat.github.io/p/perfect-vs-practical-how-to-know-when-an-it-solution-is-ready/","title":"Perfect vs Practical: How to know when an IT Solution is ready"}]